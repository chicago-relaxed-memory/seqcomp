\section{Introduction}
\label{sec:intro}

\emph{Sequentiality} is a \emph{leaky abstraction} \cite{leaky}.  For
example, sequentiality tells us that when executing
$(\PR{x}{r_1}\SEMI\PW{y}{r_2}),$ the assignment $\PR{x}{r_1}$ is executed
before $\PW{y}{r_2}$.  Thus, one might reasonably expect that the final value
of $r_1$ is independent of the initial value of $r_2$.  In most modern
languages, however, this fails to hold when the program is run concurrently
with $(\PR{y}{s}\SEMI\PW{x}{s})$,
%$\PW{x}{\PR{y}{}}$,
which copies $y$ to $x$.

In certain cases it is possible to ban concurrent access using separation
\cite{OHearn:2007:RCL:1235896.1236121}, or to accept inefficient
implementation in order to obtain sequential consistency
\cite{DBLP:conf/snapl/MarinoMMNS15}.  When these approaches are not
available, however, the humble semicolon becomes shrouded in mystery, covered
in the cloak of something known as a \emph{memory model}.  Every language has
such a model: For each read operation, it determines the set of available
values.  Compilers and runtime systems are allowed to chose any value in the
set.  To allow efficient implementation, the set must not be too small.  To
allow invariant reasoning, the set must not be too large.

For optimized concurrent languages, it is surprising difficult to define a
model that allows common compiler optimizations and hardware reorderings yet
disallows nonsense behaviors that don't arise in practice.  The latter are
commonly known as ``thin air'' behaviors \cite{DBLP:conf/esop/BattyMNPS15}.
There are only a handful of solutions, and all have deficiencies.  These can
be classified by their approach to dependency tracking (from strongest to
weakest):
\begin{itemize}
\item Syntactic dependencies
  \cite{DBLP:conf/pldi/LahavVKHD17,Boehm:2014:OGA:2618128.2618134,DBLP:conf/oopsla/VafeiadisN13,DBLP:journals/corr/abs-1804-04214}.
  These models require inefficient implementation of relaxed access.  This is
  a non-starter for safe languages like Java and Javascript, and may be an
  unacceptable cost for low-level languages like \cXI{}.
\item Semantic dependencies \cite{%
    Manson:2005:JMM:1047659.1040336, DBLP:conf/esop/JagadeesanPR10,
    DBLP:conf/popl/KangHLVD17, DBLP:journals/pacmpl/ChakrabortyV19,
    DBLP:conf/pldi/LeeCPCHLV20, promising-ldrf}. These models compute
  dependencies operationally using alternate worlds, making it impossible to
  understood a single execution in isolation; they also allow executions that
  violate temporal reasoning (see \textsection\ref{sec:related}).
\item No dependencies, as in \cXI{} \cite{DBLP:conf/esop/BattyMNPS15} and
  Javascript \cite{DBLP:journals/pacmpl/WattRP19}.  This allows thin air
  executions.
\end{itemize}

These models are all non-compositional in the sense that in order to
calculate the meaning of any thread, all threads must be known.  Using the
axiomatic approach of \cXI{}, for example, execution graphs are first
constructed for each thread, using an operational semantics that allows a
read to see any value.  The combined graphs are then filtered using a set of
acyclicity axioms that determine which reads are valid.  These axioms use
existentially defined global relations, such as memory order $\rmo$, which
must be a per-location total order on write actions.

Part of this non-compositionality is essential: In a concurrent system, the
complete set of writes is known only at top-level.  However, much of it is
incidental.  Two recent models have attempted to limit non-compositionality.
\citet{DBLP:journals/pacmpl/JagadeesanJR20} defined Pomsets with
Preconditions (\PwP), which use preconditions and logic to calculate
dependencies for a Java-like language.
\citet{DBLP:conf/esop/PaviottiCPWOB20} defined Modular Relaxed Dependencies
(\MRD), which use event structures to calculate a semantic dependency
relation ($\rsdep$).  \PwP{} is defined using (acyclic) labelled partial
orders, or \emph{pomsets} \cite{GISCHER1988199}.  \MRD{} adds a causality
axiom to \cXI{}, stating that $({\rsdep}\cup{\rrfx})$ must be acyclic.  In
both approaches, acyclicity enables inductive reasoning.

% While largely compositional, avoiding relations such as $\rmo$, \PwP{} still
% requires some top-level filtering to ensure that every read has a matching
% write---this seems unavoidable.  \MRD{} calculates $\rsdep$ compositionally,
% but otherwise suffers from the non-compositionality of \cXI{}.

While \PwP{} and \MRD{} both treat \emph{concurrency} compositionally, neither gives
a compositional account of \emph{sequentiality}.  \PwP{} uses prefixing,
adding one event at a time on the left.  \MRD{} encodes sequential
composition using continuation-passing.  In both, adding an event requires
perfect knowledge of the future.  For example, suppose that you are writing
system call code and you wish to know if you can reorder a couple of
statements.  Using \PwP{} or \MRD{}, you cannot tell whether this is possible
without having the calling code!  More formally,
\citeauthor{DBLP:journals/pacmpl/JagadeesanJR20} state the equivalence
allowing reordering independent writes as follows:
\begin{align*}
  \sem{\aLoc \GETS \aExp \SEMI \bLoc  \GETS \bExp\SEMI\aCmd} &=
  \sem{\bLoc  \GETS \bExp\SEMI \aLoc \GETS \aExp\SEMI\aCmd} \;\;\text{if}\; \aLoc\neq\bLoc
\end{align*}
This requires a quantification over all continuations $\aCmd$. This is
problematic, both from a theoretical point of view---the syntax of programs
is now mentioned in the definition of the semantics---and in practice---tools
cannot quantify over infinite sets. This problem is related to contextual
equivalence, full abstraction
\cite{DBLP:journals/tcs/Milner77,DBLP:journals/tcs/Plotkin77} and the CIU
theorem of \citet{DBLP:conf/lics/MasonT92}.  

In this paper, we show that \PwP{} can be extended with \emph{families of
  predicate transformers} (\PwT{}) to calculate sequential dependencies in a way that
is \emph{compositional} and \emph{direct}: \emph{compositional} in that the
denotation of $(\aCmd_1\SEMI \aCmd_2)$ can be computed from the denotation of
$\aCmd_1$ and the denotation of $\aCmd_2$, and \emph{direct} in that these
can be calculated independently.  With this formulation, we can show:
\begin{align*}
  \sem{\aLoc \GETS \aExp \SEMI \bLoc  \GETS \bExp} &=
  \sem{\bLoc  \GETS \bExp\SEMI \aLoc \GETS \aExp} \;\;\text{if}\; \aLoc\neq\bLoc
\end{align*}
Then the equivalence holds in any context---this form of the equivalence
enables reasoning about peephole optimizations.  Unlike prior work, \PwT{}
allows the presence or absence of a dependency can be understood in
isolation.  This enables incremental and modular validation of assumptions
about program dependencies in larger blocks of code.

Our main insight is that for language models, \emph{sequentiality} is the
hard part.  \emph{Concurrency} is easy!  Or at least, it is no more difficult
than it is for hardware.  Compilers make the difference, since they typically
do little optimization between threads.  We motivate our approach to
sequential dependencies in \textsection\ref{sec:overview} and provide formal
definitions in \textsection\ref{sec:model}.  We extend the model to include
additional features, such as address calculation and \RMW{}s, in
\textsection\ref{sec:additional}.  We further discuss related work in
\textsection\ref{sec:related}.

We extend \PwT{} to a full memory model in \textsection\ref{sec:mca}, based
on \PwP{} \cite{DBLP:journals/pacmpl/JagadeesanJR20}.
\textsection\ref{sec:results} summarizes the results for this model.  The
dependency relation calculated by \PwT{} can also be used with off-the-shelf
models.  For example, in \textsection\ref{sec:c11} we show that it can be
used as an $\rsdep$ relation for \cXI, adapting the approach of \MRD{}
\cite{DBLP:conf/esop/PaviottiCPWOB20}.  % This is only the second semantics
% to distill dependencies down to a relation compatible with the existing
% \cpp{} standard, and the first one to be fully compositional.
\textsection\ref{sec:tool} describes a tool for automatic evaluation of
litmus tests in this model.  \cXI{} allows thin-air in order to avoid
overhead in the implementation of relaxed reads.  Safe language like OCaml
\cite{Dolan:2018:BDR:3192366.3192421} have typically made the opposite choice.  Just
\PwT{} can be used to strengthen \cXI{}, it could also be used to weaken these
models, allowing optimal lowering for relaxed reads while banning thin-air.


% The
% definition is associative: the denotation of
% $((\aCmd_1\SEMI \aCmd_2)\SEMI \aCmd_3)$ is the same as the denotation of
% $(\aCmd_1\SEMI (\aCmd_2\SEMI \aCmd_3))$.  It also validates expected laws
% concerning the interaction of sequencing and conditional execution.

% To manage complexity, we have layered the definitions.  After an overview and
% discussion of related work, we
% define sequential dependencies in \textsection\ref{sec:model}.  We then add
% concurrency.  In \textsection\ref{sec:mca}, we define \PwTmca{},
% which provides a Java-like model for {multicopy-atomic} (\mca{}) hardware, similar to that of
% \citet{DBLP:journals/pacmpl/JagadeesanJR20};
% \textsection\ref{sec:results} summarizes the results for this model. In \textsection\ref{sec:c11},
% we define \PwTc{}, which models \cXI, adapting the approach of
% \citet{DBLP:conf/esop/PaviottiCPWOB20};  \textsection\ref{sec:tool} 
% describes a tool for automatic evaluation of litmus tests.  In \textsection\ref{sec:additional},
% we extend the semantics to include additional features, such as address
% calculation and \RMW{}s.  

\PwT{} has been formalized in Coq.  We have formally verified that the
sequential composition satisfies the expected monoid laws
(\reflem{lem:monoid}).  In addition we have formally verified that
\begin{math}
  \sem{\xIFTHEN{\aForm}{
      \xSEMI{\aCmd_1}{\aCmd_3}
    }{
      \xSEMI{\aCmd_2}{\aCmd_3}
    }}
  \supseteq
  \sem{\xSEMI{
      \xIFTHEN{\aForm}{\aCmd_1}{\aCmd_2}
    }{
      \aCmd_3
    }}
\end{math}
(\reflem{lem:if}\ref{lem:if:seq}).



% \begin{itemize}
% \item We provide a tool to execute litmus tests for both models.
% \item \textsection\ref{sec:sc} We prove \drfsc{} for \PwTmca{}; \MRD{} inherits the result from \rcXI.
% \item \textsection\ref{sec:arm} We prove a lowering result for \PwTmca{}.
% \item \textsection\ref{sec:additional} We extend the model to include many features.
% \end{itemize}

% % \subsection{Contributions}


% % We show how predicate transformers~\cite{DBLP:journals/cacm/Dijkstra75} can
% % be added to pomsets with
% % preconditions~\cite{DBLP:journals/pacmpl/JagadeesanJR20} to create a
% % compositional semantics for sequential composition.
% \begin{itemize}
% \item \S\ref{sec:model} presents the basic model, which satisfies many
%   desiderata, but not all.
% \item \S\ref{sec:arm} shows two approaches for efficient implementation on
%   Arm.  The first uses a suboptimal lowering for acquiring reads.  The second
%   uses an optimal lowering, but requires a nontrivial change to the
%   definition of sequential composition.
% \item \S\ref{sec:additional} generalizes the basic semantics of read and write
%   to validate compiler optimizations.
% \end{itemize}
% % Acknowledgements go here, once we're not double-blinded.
% % The definitions in this paper have been formalized in Agda.
% Because it is closely related, we expect that the memory-model results of
% \cite{DBLP:journals/pacmpl/JagadeesanJR20} apply to our model, including
% compositional reasoning for temporal safety properties and {local} \drfsc{}
% as in \cite{Dolan:2018:BDR:3192366.3192421,DBLP:conf/ppopp/DongolJR19,promising-ldrf}.
% % In \textsection\ref{sec:arm}, we provide an alternative proof strategy for
% % efficient compilation to \armeight{}, which improves upon that of
% % \cite{DBLP:journals/pacmpl/JagadeesanJR20} by using a recent alternative
% % characterization of \armeight{}.

\section{Overview}
\label{sec:overview}

This paper is about the interaction of two of the fundamental building blocks
of computing: sequential composition and mutable state. One would like to
think that these are well-worn topics, where every issue has been settled,
but this is not the case.

\subsection{Sequential Composition} %Predicate Transformers}

Novice programmers are taught \emph{sequential abstraction}: that the
program $\aCmd_1\SEMI \aCmd_2$ executes $\aCmd_1$ before $\aCmd_2$.  Since
the late 1960s, we've been able to explain this using logic
\citep{Hoare:1969:ABC:363235.363259}.  In
\citeauthor{DBLP:journals/cacm/Dijkstra75}'s
[\citeyear{DBLP:journals/cacm/Dijkstra75}] formulation, we think of programs
as \emph{predicate transformers}, where predicates describe the state of
memory in the system.  In the calculus of weakest preconditions, programs map
postconditions to preconditions.  We recall the definition of
$\fwp{\aCmd}{\bForm}$ for loop-free code below (where $\aReg$--$\bReg$ range over thread-local \emph{registers}
and $\aExp$--$\bExp$ range over side-effect-free
\emph{expressions}). 
% \begin{multicols}{3}
%   \begin{enumerate}[,label=(\textsc{d}\arabic*),ref=\textsc{d}\arabic*]
%   \item \label{wp-let}
%     $\fwp{\LET{\aReg}{\aExp}}{\bForm} = \bForm[\aExp/\aReg]$
%   \item
%     \makebox[0cm][l]{
%     \begin{math}
%       \fwp{\IF{\aExp}\THEN \aCmd_1\ELSE \aCmd_2\FI}{\bForm}= {}
%     \end{math}
%     \begin{math}
%       ((\aExp{\ne}0) \limplies \fwp{\aCmd_1}{\bForm}) \land
%       ((\aExp{=}0) \limplies \fwp{\aCmd_2}{\bForm})
%     \end{math}}
%     % \item 
%     %   \begin{math}
%     %     \fwp{\ABORT}{\bForm} = \FALSE
%     %   \end{math}
%     %   \stepcounter{enumi}
%     % \item[] \labeltext[\textsc{d}2]{}{wp-assign}
%     %   \begin{enumerate}[leftmargin=0pt]
%     %   \item \label{wp-write}
%     %     $\fwp{\PW{\aLoc}{\aExp}}{\bForm} = \bForm[\aExp/\aLoc]$
%     %   \item \label{wp-let}
%     %     $\fwp{\LET{\aReg}{\aExp}}{\bForm} = \bForm[\aExp/\aReg]$
%     %   \item \label{wp-read}
%     %     $\fwp{\PR{\aLoc}{\aReg}}{\bForm} = \aLoc{=}\aReg\limplies\bForm$ %\bForm[\aLoc/\aReg]$ %
%     %   \end{enumerate}
%     % \item 
%     %   \begin{math}
%     %     \fwp{\LET{x}{\aExp}}{\bForm} = \bForm[\aExp/x]
%     %   \end{math}
%   \columnbreak
%   \item
%     \makebox[0cm][l]{
%     \begin{math}
%       \fwp{\aCmd_1;\aCmd_2}{\bForm} = \fwp{\aCmd_1}{\fwp{\aCmd_2}{\bForm}}
%     \end{math}}
%   \columnbreak
%   \item
%     \begin{math}
%       \fwp{\SKIP}{\bForm} = \bForm
%     \end{math}
%   \end{enumerate}
% \end{multicols}
\begin{gather*}
  \fwp{\LET{\aReg}{\aExp}}{\bForm} = \bForm[\aExp/\aReg]
  \qquad
  \fwp{\aCmd_1;\aCmd_2}{\bForm} = \fwp{\aCmd_1}{\fwp{\aCmd_2}{\bForm}}
  \qquad
  \fwp{\SKIP}{\bForm} = \bForm
  \\
  \fwp{\IF{\aExp}\THEN \aCmd_1\ELSE \aCmd_2\FI}{\bForm}= 
  ((\aExp{\ne}0) \limplies \fwp{\aCmd_1}{\bForm}) \land
  ((\aExp{=}0) \limplies \fwp{\aCmd_2}{\bForm})
\end{gather*}    
Without loops, the Hoare triple
$\hoare{\aForm}{\aCmd}{\bForm}$ holds exactly when $\aForm \limplies
\fwp{\aCmd}{\bForm}$.
%% We have split
%% \citeauthor{DBLP:journals/cacm/Dijkstra75}'s rule for assignment
%% \eqref{wp-assign} into three cases. In our notation,
%% $\aReg$--$\bReg$ range over thread-local registers, which may be assigned at
%% most once,
%% $\aLoc$--$\cLoc$ range over shared memory references, and
%% $\aExp$--$\bExp$ range over thread-local expressions, which do \emph{not}
%% include
%% $\aLoc$--$\cLoc$.\footnote{Under these assumptions, \eqref{wp-read} is
%%   equivalent to $\fwp{\PR{\aLoc}{\aReg}}{\bForm} = \bForm[\aLoc/\aReg]$.}
%%
This is an elegant explanation of sequential computation in a sequential
context. Note that the assignment rule %\ref{wp-let}
is sound because a read from a thread-local
register must be fulfilled by a preceding write in the same thread.
In a concurrent context, with shared variables
($\aLoc$--$\cLoc$), the obvious generalization of the assignment rule
% \eqref{wp-let}
for reads,
$\fwp{\LET{\aReg}{\aLoc}}{\bForm} = \bForm[\aLoc/\aReg]$,
% \begin{multicols}{2}
% \begin{itemize}
% \item[{\labeltext[\textsc{d}1a]{(\textsc{d}1a)}{wp-write}}] $\fwp{\LET{\aLoc}{\aExp}}{\bForm} = \bForm[\aExp/\aLoc]$
% \item[{\labeltext[\textsc{d}1b]{(\textsc{d}1b)}{wp-read}}] $\fwp{\LET{\aReg}{\aLoc}}{\bForm} = \bForm[\aLoc/\aReg]$
% \end{itemize}
% \end{multicols}
% \allowbreak
% \noindent
is unsound!
% (Expressions
% % $\aExp$--$\bExp$
% do \emph{not} include shared variables.) % $\aLoc$--$\cLoc$.)
In particular, a read from a shared memory location may be fulfilled by a write
in another thread.


In this paper we answer the following question: what does sequential
composition mean in a concurrent context?  An acceptable answer must satisfy
several desiderata:
\begin{enumerate}
\item\label{too:cold} it should not impose too much order, overconstraining the implementation,
\item\label{too:hot} it should not impose too little order, allowing bogus executions, and
\item it should be \emph{compositional} and \emph{direct}, as described in \textsection\ref{sec:intro}.
\end{enumerate}
Memory models differ in how they navigate between desiderata \ref{too:cold}
and \ref{too:hot}.  In one direction there are both more valid compiler
optimizations and also more potentially dubious executions, in the other
direction, less of both.  To understand the tradeoffs, one must first
understand the underlying hardware and compilers.
% Existing work studies program logics over non-composition operational or
% axiomatic models
% \cite{OHearn:2007:RCL:1235896.1236121,DBLP:conf/oopsla/VafeiadisN13,DBLP:conf/esop/SvendsenPDLV18}.
%
% Existing approaches either assume exclusive access, as in concurrent
% separation logic \cite{OHearn:2007:RCL:1235896.1236121}, or abandon the
% logical approach altogether, as in the pomset model of
% \citet{DBLP:journals/corr/abs-1804-04214}, which enforces syntactic
% dependencies.  This leaves open the question of how to apply logic to racy
% programs without overconstraining the implementation.  


% Assuming all variables are initialized to $0$,
% $\PW{x}{1}\SEMI\PW{y}{1}$ can be distinguished from $\PW{y}{1}\SEMI\PW{x}{1}$
% by the concurrent observer $\PR{x}{r}\SEMI\PR{y}{s}$

% Microprocessors and compilers do not execute this way, but they attempt to
% preserve the abstraction, at least for sequential code.  For concurrent code,
% all bets are off.

% \citet{DBLP:conf/snapl/MarinoMMNS15} argue that the ``silently shifting
% semicolon'' is problematic for programmers, and thus concurrent languages
% should guarantee sequential abstraction.  But processor and language
% implementors balk at the prospect, due to significant costs.

% These leaves programmers in a bit of pickle: What does that semicolon mean?

% The last decade has seen a slew of research in ``relaxed memory models,''
% which attempt to explain concurrent execution.  The models are mostly
% operational, sometimes with an additional series of axioms to eliminate
% ``bad'' executions.  These models have limited modularity properties,
% typically requiring whole program analysis.

% In this paper, we attempt to 





\subsection{Memory Models}

For single-threaded programs, memory can be thought of as you might
expect: programs write to, and read from, memory references.
This can be thought of as a total order over memory actions ($\xwki$),
where each read has a matching \emph{fulfilling} write ($\xrf$),
for example:
\begin{gather*}
  \THREAD{\PW{x}{0}\SEMI \PW{x}{1}\SEMI \PW{y}{2}\SEMI
    \PR{y}{r}\SEMI \PR{x}{s}}
  \\[-.4ex]
  \hbox{\begin{tikzinline}[node distance=1.5em]
      \event{wx0}{\DW{x}{0}}{}
      \event{wx1}{\DW{x}{1}}{right=of wx0}
      \event{wy2}{\DW{y}{2}}{right=of wx1}
      \event{ry2}{\DR{y}{2}}{right=of wy2}
      \event{rx1}{\DR{x}{1}}{right=of ry2}
      \rf[out=15,in=165]{wy2}{ry2}
      \rf[out=15,in=165]{wx1}{rx1}
      \wki{wx0}{wx1}
      \wki{wx1}{wy2}
      \wki{wy2}{ry2}
      \wki{ry2}{rx1}
    \end{tikzinline}}
\end{gather*}

% (In examples, $\aReg$--$\bReg$ range over thread-local registers and $\aLoc$-$\cLoc$
% range over shared memory references.)

This model extends naturally to the case of shared-memory concurrency, leading to a \emph{sequentially consistent}
semantics \cite{Lamport:1979:MMC:1311099.1311750}, in which \emph{program order} inside a thread implies
a total \emph{causal order} between read and write events, for example
(where $\SEMI\!$ has higher precedence than $\PAR$):
\begin{gather*}
  \THREAD{\PW{x}{0}\SEMI \PW{x}{1}\SEMI \PW{y}{2}}
  \PAR
  \THREAD{\PR{y}{r}\SEMI \PR{x}{s}}
  \\[-.4ex]
  \hbox{\begin{tikzinline}[node distance=1.5em]
      \event{wx0}{\DW{x}{0}}{}
      \event{wx1}{\DW{x}{1}}{right=of wx0}
      \event{wy2}{\DW{y}{2}}{right=of wx1}
      \event{ry2}{\DR{y}{2}}{right=3em of wy2}
      \event{rx1}{\DR{x}{1}}{right=of ry2}
      \rf[out=15,in=165]{wy2}{ry2}
      \rf[out=15,in=165]{wx1}{rx1}
      \wki{wx0}{wx1}
      \wki{wx1}{wy2}
      \wki{ry2}{rx1}
    \end{tikzinline}}
\end{gather*}
We can represent such an execution as a labelled partial order, or
\emph{pomset} \cite{DBLP:conf/lop/Pratt85,GISCHER1988199}.  A program may give rise to many
executions, each reflecting a different interleaving of the threads.

Unfortunately, this model does not compile efficiently to commodity
hardware, resulting in a 37--73\% increase in CPU time on Arm8~\cite{Liu:2019:ASC:3314221.3314611} and,
hence, in power consumption.  Developers of software and compilers have
therefore been faced with a difficult trade-off, between an elegant
model of memory, and its impact on resource usage (such as size of
data centers, electricity bills and carbon footprint). Unsurprisingly,
many have chosen to prioritize efficiency over elegance.

This has led to \emph{relaxed memory models}, in which the requirement of
sequential consistency is weakened to only apply \emph{per-location}. This allows executions that
are inconsistent with program order, such as the following, which contains an
\emph{antidependency} $(\xwk)$:
\begin{gather*}
  \THREAD{\PW{x}{0}\SEMI \PW{x}{1}\SEMI \PW{y}{2}}
  \PAR
  \THREAD{\PR{y}{r}\SEMI \PR{x}{s}}
  \\[-.4ex]
  \hbox{\begin{tikzinline}[node distance=1.5em]
      \event{wx0}{\DW{x}{0}}{}
      \event{wx1}{\DW{x}{1}}{right=of wx0}
      \event{wy2}{\DW{y}{2}}{right=of wx1}
      \event{ry2}{\DR{y}{2}}{right=3em of wy2}
      \event{rx0}{\DR{x}{0}}{right=of ry2}
      \rf[out=15,in=165]{wy2}{ry2}
      \rf[out=15,in=165]{wx0}{rx0}
      \wki{wx0}{wx1}
      \wk[out=-165,in=-15]{rx0}{wx1}
    \end{tikzinline}}
\end{gather*}
% Reads must be ordered between blocking writes, creating
% \emph{antidependencies} such as $\DRP{x}{0}\xwk\DWP{x}{1}$.

In such models, the causal order between events is important, and includes
control and data dependencies ($\xpo$) to avoid paradoxical ``out of thin
air'' examples such as the following.  (We routinely elide initializing
writes when they are uninteresting.)
\begin{gather*}
  \THREAD{\PR{x}{r}\SEMI \IF r \THEN \PW{y}{1} \FI}
  \PAR
  \THREAD{\PR{y}{s}\SEMI \PW{x}{s}}
  \\[-.4ex]
  \hbox{\begin{tikzinline}[node distance=1.5em]
      \event{rx1}{\DR{x}{1}}{}
      \event{wy1}{\DW{y}{1}}{right=of rx1}
      \event{ry1}{\DR{y}{1}}{right=3em of wy1}
      \event{wx1}{\DW{x}{1}}{right=of ry1}
      \rf[out=15,in=165]{wy1}{ry1}
      \rf[out=-165,in=-15]{wx1}{rx1}
      \po{rx1}{wy1}
      \po{ry1}{wx1}
    \end{tikzinline}}
\end{gather*}
This candidate execution forms a cycle in causal order, so is disallowed,
but this depends crucially on the control dependency
from $(\DR{x}{1})$ to $(\DW{y}{1})$, and the data dependency
from $(\DR{y}{1})$ to $(\DW{x}{1})$. If either is missing, then this execution
is acyclic and hence allowed. For example dropping the control dependency
results in:
\begin{gather*}
  \THREAD{\PR{x}{r}\SEMI \PW{y}{1}}
  \PAR
  \THREAD{\PR{y}{s}\SEMI \PW{x}{s}}
  \\[-.4ex]
  \hbox{\begin{tikzinline}[node distance=1.5em]
      \event{rx1}{\DR{x}{1}}{}
      \event{wy1}{\DW{y}{1}}{right=of rx1}
      \event{ry1}{\DR{y}{1}}{right=3em of wy1}
      \event{wx1}{\DW{x}{1}}{right=of ry1}
      \rf[out=15,in=165]{wy1}{ry1}
      \rf[out=-165,in=-15]{wx1}{rx1}
      \po{ry1}{wx1}
    \end{tikzinline}}
\end{gather*}

While syntactic dependency calculation suffices for hardware models, it is
not preserved by common compiler optimizations. For example, if we calculate
control dependencies syntactically, then there is a dependency from
$(\DR{x}{1})$ to $(\DW{y}{1})$, and therefore a cycle in, the candidate
execution:
\begin{gather*}
  \THREAD{\PR{x}{r}\SEMI \IF r \THEN \PW{y}{1} \ELSE \PW{y}{1} \FI}
  \PAR
  \THREAD{\PR{y}{s}\SEMI \PW{x}{s}}
  \\[-.4ex]
  \hbox{\begin{tikzinline}[node distance=1.5em]
      \event{rx1}{\DR{x}{1}}{}
      \event{wy1}{\DW{y}{1}}{right=of rx1}
      \event{ry1}{\DR{y}{1}}{right=3em of wy1}
      \event{wx1}{\DW{x}{1}}{right=of ry1}
      \rf[out=15,in=165]{wy1}{ry1}
      \rf[out=-165,in=-15]{wx1}{rx1}
      \po{rx1}{wy1}
      \po{ry1}{wx1}
    \end{tikzinline}}
\end{gather*}
A compiler may lift the assignment $\PW{y}{1}$ out of the conditional,
thus removing the dependency.

To address this, \citet{DBLP:journals/pacmpl/JagadeesanJR20} introduced
\emph{Pomsets with Preconditions} (\PwP{}), where events are labeled with logical
formulae.  Nontrivial preconditions are introduced by store actions (modeling
data dependencies) and conditionals (modeling control dependencies):
\begin{gather*}
  \IF{s{>}0} \THEN \PW{z}{r{*}(s{-}1)} \FI
  \\
  \hbox{\begin{tikzinline}[node distance=1.5em]
      \event{wz0}{(s {>} 0) \land (r{*}(s{-}1)){=}0 \bigmid \DW{z}{0}}{}
    \end{tikzinline}}
\end{gather*}
In this diagram, $(s {>} 0)$ is a control dependency and $(r{*}(s{-}1)){=}0$ is a
data dependency. Preconditions are updated as events are prepended (we assume the
usual precedence for logical operators): 
\begin{gather*}
  %\tag{$\dagger$}\label{eq1}
  \PR{x}{r}\SEMI \PR{y}{s}\SEMI \IF{s{>}0} \THEN \PW{z}{r{*}(s{-}1)} \FI
  \\
  \hbox{\begin{tikzinline}[node distance=1.5em]
      \event{rx0}{\DR{x}{1}}{}
      \event{ry0}{\DR{y}{1}}{right=of rx0}
      \event{wz0}{(1{=}s) \limplies (s{>}0) \land (r{*}(s{-}1)){=}0 \bigmid \DW{z}{0}}{right=of ry0}
      \po{ry0}{wz0}
    \end{tikzinline}}
\end{gather*}
In this diagram there are two reads.  As evidenced by the arrow, the read of
$y$ is ordered before the write, reflecting possible dependency; the read of
$x$ is not, reflecting independency.  The dependent read of $y$ allows the
precondition of the write to weaken: now the old precondition need only be
satisfied assuming the hypothesis $(1{=}s)$.  The independent read of $x$
allows no such weakening.  Nonetheless, the precondition of the write is now
a tautology, and so can be elided in the diagram.

We can complete the execution by adding the required writes:
\begin{gather*}
  \PW{x}{1}
  \SEMI
  \PW{y}{1}
  \PAR
  \PR{x}{r}\SEMI \PR{y}{s}\SEMI \IF{s{>}0} \THEN \PW{z}{r{*}(s{-}1)} \FI
  \\
  \hbox{\begin{tikzinline}[node distance=1.5em]
      \event{wx}{\DW{x}{1}}{}
      \event{wy}{\DW{y}{1}}{right=of wx}
      \event{rx}{\DR{x}{1}}{right=3em of wy}
      \event{ry}{\DR{y}{1}}{right=of rx}
      \event{wz}{\DW{z}{0}}{right=of ry}
      \po{ry}{wz}
      \rf[out=10,in=170]{wx}{rx}
      \rf[out=10,in=170]{wy}{ry}
    \end{tikzinline}}
\end{gather*}
In order for a \PwP{} to be \emph{complete}, all preconditions must be
tautologies and all reads must be fulfilled by matching writes.  The first
requirement captures the sequential semantics.  The second requirement
captures the concurrent semantics.  These correspond to two views of memory
for each thread: thread-local and global.  In a
\emph{multicopy-atomic} (\mca{}) architecture, there is only one global view,
shared by all processors, which is neatly captured by the order of the
pomset.

An untaken conditional produces no events.  \PwP{} models this by including
the empty pomset in the semantics of every program fragment.  To then ensure
that $\SKIP$ is not a refinement of $\PW{x}{1}$, \PwP{} include the
\emph{termination} action, $\aTerm{}$, which we have elided in the examples
above.

\subsection{Predicate Transformers For Relaxed Memory}

\PwP{} shows how the logical approach to sequential dependency calculation
can be mixed into a relaxed memory model.  Our contribution is to extend
\PwP{} with predicate transformers to arrive at a model of sequential
composition.  Predicate transformers are a good fit for logical models of
dependency calculation, since both are concerned with
preconditions.% and how they are
% transformed by sequential composition.

% Instead, their model uses \emph{prefixing}, which
% requires that the model is built from right to left: events are prepended one
% at a time, with perfect knowledge of the future.  This makes reasoning about
% sequential program fragments difficult.  For example,
% \citeauthor{DBLP:journals/pacmpl/JagadeesanJR20} state the equivalence
% allowing reordering independent writes as follows,
% \begin{align*}
%   \sem{\aLoc \GETS \aExp \SEMI \bLoc  \GETS \bExp\SEMI\aCmd} &=
%   \sem{\bLoc  \GETS \bExp\SEMI \aLoc \GETS \aExp\SEMI\aCmd} \;\;\text{if}\; \aLoc\neq\bLoc
% \end{align*}
% where $\aCmd$ is the entire future computation!  By formalizing sequential
% composition, we can show:
% \begin{align*}
%   \sem{\aLoc \GETS \aExp \SEMI \bLoc  \GETS \bExp} &=
%   \sem{\bLoc  \GETS \bExp\SEMI \aLoc \GETS \aExp} \;\;\text{if}\; \aLoc\neq\bLoc
% \end{align*}
% Then the equivalence holds in any
% %% (sequential)
% context.


% While existing models of relaxed memory have detailed treatments of parallel composition,
% they often give sequential composition little attention, either ignoring it altogether,
% or treating it operationally with its usual small-step semantics. This paper
% investigates how existing models of sequential composition interact with relaxed memory.

% \begin{figure*}
%   \input{fig-sp.tex}
%   \caption{Weakest precondition semantics}
%   \label{fig:sp}
% \end{figure*}

% Our approach follows that of weakest precondition semantics of
% \citet{DBLP:journals/cacm/Dijkstra75}, which provides an alternative
% characterization of Hoare logic \citep{Hoare:1969:ABC:363235.363259} by
% mapping postconditions to preconditions. We recall the definition of
% $\fwp{\aCmd}{\bForm}$ for loop-free code below. % in Figure~\ref{fig:sp}
% \begin{itemize}
% \item
%   \begin{math}
%     \fwp{\SKIP}{\bForm} = \bForm
%   \end{math}
% % \item 
% %   \begin{math}
% %     \fwp{\ABORT}{\bForm} = \FALSE
% %   \end{math}
% \item
%   \begin{math}
%     \fwp{\LET{r}{\aExp}}{\bForm} = \bForm[\aExp/r]
%   \end{math}
% \item
%   \begin{math}
%     \fwp{\aCmd_1;\aCmd_2}{\bForm} = \fwp{\aCmd_1}{\fwp{\aCmd_2}{\bForm}}
%   \end{math}
% \item
%   \begin{math}
%     \fwp{\IF{\aExp}\THEN \aCmd_1\ELSE \aCmd_2\FI}{\bForm}= {}
%   \end{math}
%   \\
%   \begin{math}
%     ((\aExp{\ne}0) \limplies \fwp{\aCmd_1}{\bForm}) \land
%     ((\aExp{=}0) \limplies \fwp{\aCmd_2}{\bForm})
%   \end{math}
% \end{itemize}
% The rule we are most interested
% in is the one for sequential composition, which maps sequential composition of programs
% to function composition of predicate transformers.


Our first attempt is to associate a
predicate transformer with each pomset. We visualize this in diagrams by
showing how $\bForm$ is transformed, for example:
\begin{align*}
  \begin{gathered}
    \PR{x}{r}
    \\
    \hbox{\begin{tikzinline}[node distance=1ex and 1.5em]
        \event{rx0}{\DR{x}{0}}{}
        \xform{rx0d}{(0{=}r) \limplies \bForm}{right=of rx0}
        %\xo{rx0}{rx0d}
      \end{tikzinline}}
  \end{gathered}
  &&
  \begin{gathered}
    \PR{y}{s}
    \\
    \hbox{\begin{tikzinline}[node distance=1ex and 1.5em]
        \event{ry0}{\DR{y}{0}}{}
        \xform{ry0d}{(0{=}s) \limplies \bForm}{right=of ry0}
        %\xo{ry0}{ry0d}
      \end{tikzinline}}
  \end{gathered}
  &&
  \begin{gathered}
    \IF{s{<}1} \THEN \PW{z}{r{*}s} \FI
    \\
    \hbox{\begin{tikzinline}[node distance=1ex and 1.5em]
        \event{wz0}{(s {<} 1) \land (r{*}s){=}0 \bigmid \DW{z}{0}}{}
        \xform{wz0d}{\bForm[r{*}s/z]}{right=of wz0}
        %\xo{wz0}{wz0d}
      \end{tikzinline}}
  \end{gathered}
\end{align*}
The predicate transformer for a write $\PW{z}{\aExp}$ matches
\citeauthor{DBLP:journals/cacm/Dijkstra75}: taking $\bForm$ to
$\bForm[\aExp/z]$.  For a read $\PR{\aLoc}{\aReg}$, however,
\citeauthor{DBLP:journals/cacm/Dijkstra75} would transform $\bForm$ to
$\bForm[x/r]$, which is equivalent to $(x{=}r) \limplies \bForm$ under the
assumption that registers are assigned at most once.  Instead, we use
$(0{=}r) \limplies \bForm$, reflecting the fact that $0$ may come from a
concurrent write.  The obligation to find a matching write is moved from the
sequential semantics of \emph{substitution} and \emph{implication} to the
concurrent semantics of \emph{fulfillment}.

% In the rightmost program above, the write to $z$ affects the shared store, not the
% local state of the thread, therefore we assign it the identity transformer.

For a sequentially consistent semantics, sequential composition is
straightforward: we apply each predicate transformer to subsequent
preconditions, composing the predicate transformers.
\begin{gather*}
  \PR{x}{r}\SEMI \PR{y}{s}\SEMI \IF{s{<}1} \THEN \PW{z}{r{*}s} \FI
  \\
  \hbox{\begin{tikzinline}[node distance=1ex and 1.5em]
      \event{rx0}{\DR{x}{0}}{}
      \event{ry0}{\DR{y}{0}}{right=of rx0}
      \event{wz0}{(0{=}r) \limplies (0{=}s) \limplies (s {<} 1) \land (r{*}s){=}0 \bigmid \DW{z}{0}}{right=of ry0}
      \xform{rx0ry0d}{(0{=}r) \limplies(0{=}s) \limplies \bForm[r{*}s/z]}{right=of wz0}
      \wki{rx0}{ry0}
      \wki{ry0}{wz0}
      %\xo{rx0}{rx0ry0d}
      %\xo[out=155,in=15]{ry0}{rx0ry0d}
      % \xo{wz0}{rx0ry0d}
    \end{tikzinline}}
\end{gather*}
This works for the sequentially consistent case, but needs to be
weakened for the relaxed case.

The key observation of this paper is
that rather than working with one predicate transformer, we should
work with a \emph{family} of predicate transformers, indexed by sets
of events.
For example, for single-event pomsets, there are two predicate
transformers, since there are two subsets of any one-element set.
The \emph{independent}
transformer is indexed by the empty set, whereas the \emph{dependent}
transformer is indexed by the singleton.
We visualize this by including more than one transformed predicate,
with a dotted edge leading to the dependent one ($\xxo$). For example:
\begin{align*}
  \begin{gathered}
    \PR{x}{r}
    \\
    \hbox{\begin{tikzinline}[node distance=1ex and 1.5em]
        \event{rx0}{\DR{x}{0}}{}
        \xform{rx0i}{\bForm}{left=.4em of rx0}
        \xform{rx0d}{(0{=}r) \limplies \bForm}{right=of rx0}
        \xo{rx0}{rx0d}
      \end{tikzinline}}
  \end{gathered}
  &&
  \begin{gathered}
    \PR{y}{s}
    \\
    \hbox{\begin{tikzinline}[node distance=1ex and 1.5em]
        \event{ry0}{\DR{y}{0}}{}
        \xform{ry0i}{\bForm}{left=.4em of ry0}
        \xform{ry0d}{(0{=}s) \limplies \bForm}{right=of ry0}
        \xo{ry0}{ry0d}
      \end{tikzinline}}
  \end{gathered}
  % &&
  % \begin{gathered}
  %   \IF{s{<}1} \THEN \PW{z}{r{*}s} \FI
  %   \\
  %   \hbox{\begin{tikzinline}[node distance=1ex and1.5em]
  %     \event{wz0}{(s {<} 1) \land (r{*}s){=}0 \bigmid \DW{z}{0}}{}
  %     \xform{wz0i}{\bForm}{left=.4em of wz0}
  %     \xform{wz0d}{\bForm}{right=of wz0}
  %     \xo{wz0}{wz0d}
  %   \end{tikzinline}}
  % \end{gathered}
\end{align*}
The model of sequential composition then picks which
predicate transformer to apply to an event's precondition by picking
the one indexed by all the events before it in causal order.

For example, we can recover the expected semantics
for \eqref{eq1} by choosing
the predicate transformer which is independent of $(\DR x0)$
but dependent on $(\DR y0)$, which is the transformer
which maps $\bForm$ to $(0{=}s) \limplies \bForm$.
(In subsequent diagrams, we only show predicate transformers for reads.)
\begin{gather*}
  \PR{x}{r}\SEMI \PR{y}{s}\SEMI \IF{s{<}1} \THEN \PW{z}{r{*}s} \FI
  \\
  \hbox{\begin{tikzinline}[node distance=2ex and 1.2em]
      \xform{wz0i}{\bForm}{}
      \xform{rx0d}{(0{=}r) \limplies \bForm}{right=of wz0i}
      \xform{rx0ry0d}{(0{=}r) \limplies(0{=}s) \limplies \bForm}{right=of rx0d}
      \xform{ry0d}{(0{=}s) \limplies \bForm}{right=of rx0ry0d}
      \event{rx0}{\DR{x}{0}}{above left=of rx0ry0d}
      \event{ry0}{\DR{y}{0}}{above right=of rx0ry0d}
      \event{wz0}{(0{=}s) \limplies (s {<} 1) \land (r{*}s){=}0 \bigmid \DW{z}{0}}{right=of ry0}
      \po{ry0}{wz0}
      \xo{rx0}{rx0d}
      \xo{ry0}{ry0d}
      \xo{rx0}{rx0ry0d}
      \xo{ry0}{rx0ry0d}
    \end{tikzinline}}
\end{gather*}
In the diagram, the dotted lines indicate set inclusion into the index of the transformer-family.
As a quick correctness test, we can see that sequential composition is
associative in this case, since it does not matter whether we
associate to the left, with the intermediate step eliding $\DWP{z}{0}$ in the
diagram above,
% \begin{gather*}
%   \PR{x}{r}\SEMI \PR{y}{s}
%   \\
%   \hbox{\begin{tikzinline}[node distance=2ex and 1.5em]
%       \xform{rx0d}{(0{=}r) \limplies \bForm}{}
%       \event{rx0}{\DR{x}{0}}{right=of rx0d}
%       \xform{rx0ry0d}{(0{=}r) \limplies(0{=}s) \limplies \bForm}{right=of rx0}
%       \event{ry0}{\DR{y}{0}}{right=of rx0ry0d}
%       \xform{ry0d}{(0{=}s) \limplies \bForm}{right=of ry0}
%       \xform{wz0i}{\bForm}{left=of rx0d}
%       \xo{rx0}{rx0d}
%       \xo{ry0}{ry0d}
%       \xo{rx0}{rx0ry0d}
%       \xo{ry0}{rx0ry0d}
%     \end{tikzinline}}
% \end{gather*}
or to the right, with the intermediate step:
\begin{gather*}
  \PR{y}{s}\SEMI \IF{s{<}1} \THEN \PW{z}{r{*}s} \FI
  \\
  \hbox{\begin{tikzinline}[node distance=1ex and 1.5em]
      \event{ry0}{\DR{y}{0}}{}
      \event{wz0}{(0{=}s) \limplies (s {<} 1) \land (r{*}s){=}0 \bigmid \DW{z}{0}}{right=of ry0}
      \xform{ry0d}{(0{=}s) \limplies \bForm}{left=of ry0}
      \xform{wz0i}{\bForm}{left=of ry0d}
      \po{ry0}{wz0}
      \xo{ry0}{ry0d}
    \end{tikzinline}}
\end{gather*}
This is an instance of the general result that sequential composition forms a monoid.

