\section{Introduction}

This paper is about the interaction of two of the fundamental building
blocks of computing: memory and sequential composition. One would like
to think that these are well-worn topics, where every issue has been
settled, but this is sadly not the case.

\subsection{Memory}

For single-threaded programs, memory can be thought of as you might
expect: programs write to, and read from, memory references.
This can be thought of as a total order of reads and writes,
where each read has a matching \emph{fulfilling} write,
for example:
  \begin{gather*}
    \THREAD{x\GETS0\SEMI x\GETS1\SEMI y\GETS2\SEMI
    r\GETS y\SEMI s\GETS x}
    \\[-.4ex]
    \nonumber
    \hbox{\begin{tikzinline}[node distance=1.5em]
        \event{wx0}{\DW{x}{0}}{}
        \event{wx1}{\DW{x}{1}}{right=of wx0}
        \event{wy2}{\DW{y}{2}}{right=of wx1}
        \event{ry2}{\DR{y}{2}}{right=of wy2}
        \event{rx1}{\DR{x}{1}}{right=of ry2}
        \rf[out=20,in=160]{wy2}{ry2}
        \rf[out=20,in=160]{wx1}{rx1}
        \wk{wx0}{wx1}
        \wk{wx1}{wy2}
        \wk{wy2}{ry2}
        \wk{ry2}{rx1}
      \end{tikzinline}}
  \end{gather*}
This model naturally extends to the case of shared-memory concurrency
in a natural way, leading to a \emph{sequentially consistent}
semantics, in which \emph{program order} inside a thread implies
a total \emph{causal order} between read and write events, for example:
  \begin{gather*}
    \THREAD{x\GETS0\SEMI x\GETS1\SEMI y\GETS2}
    \PAR
    \THREAD{r\GETS y\SEMI s\GETS x}
    \\[-.4ex]
    \nonumber
    \hbox{\begin{tikzinline}[node distance=1.5em]
        \event{wx0}{\DW{x}{0}}{}
        \event{wx1}{\DW{x}{1}}{right=of wx0}
        \event{wy2}{\DW{y}{2}}{right=of wx1}
        \event{ry2}{\DR{y}{2}}{right=of wy2}
        \event{rx1}{\DR{x}{1}}{right=of ry2}
        \rf[out=20,in=160]{wy2}{ry2}
        \rf[out=20,in=160]{wx1}{rx1}
        \wk{wx0}{wx1}
        \wk{wx1}{wy2}
        \wk{ry2}{rx1}
      \end{tikzinline}}
  \end{gather*}
Unfortunately, this model does not compile efficiently to commodity
hardware, resulting in a X\% increase in CPU time~\cite{???}, and
hence power consumption.  Developers of software and compilers have
therefore been faced with a difficult trade-off, between an elegant
model of memory, and its impact on resource usage (such as size of
data centres, electricity bills and carbon footprint). Unsurprisingly,
many have chosen to prioritize efficiency over elegance.

This has led to \emph{relaxed memory models}, in which the requirement of
sequential consistency is weakened to only apply \emph{per-location} and not globally
over the whole progfram. This allows executions which
are inconsistent with program order, such as:
  \begin{gather*}
    \THREAD{x\GETS0\SEMI x\GETS1\SEMI y\GETS2}
    \PAR
    \THREAD{r\GETS y\SEMI s\GETS x}
    \\[-.4ex]
    \nonumber
    \hbox{\begin{tikzinline}[node distance=1.5em]
        \event{wx0}{\DW{x}{0}}{}
        \event{wx1}{\DW{x}{1}}{right=of wx0}
        \event{wy2}{\DW{y}{2}}{right=of wx1}
        \event{ry2}{\DR{y}{2}}{right=of wy2}
        \event{rx0}{\DR{x}{0}}{right=of ry2}
        \rf[out=20,in=160]{wy2}{ry2}
        \rf[out=15,in=165]{wx0}{rx0}
        \wk{wx0}{wx1}
        \wk[out=200,in=340]{rx0}{wx1}
      \end{tikzinline}}
  \end{gather*}
In such models, the causal order between events is important,
and includes control and data dependencies, to avoid
paradoxical ``out of thin air'' examples such as:
  \begin{gather*}
    \THREAD{r\GETS x\SEMI \IF r \THEN y\GETS1 \FI}
    \PAR
    \THREAD{s\GETS y\SEMI x\GETS s}
    \\[-.4ex]
    \nonumber
    \hbox{\begin{tikzinline}[node distance=1.5em]
        \event{rx1}{\DR{x}{1}}{}
        \event{wy1}{\DW{y}{1}}{right=of rx1}
        \event{ry1}{\DR{y}{1}}{right=of wy1}
        \event{wx1}{\DW{x}{1}}{right=of ry1}
        \rf[out=20,in=160]{wy1}{ry1}
        \rf[out=160,in=20]{wx1}{rx1}
        \po{rx1}{wy1}
        \po{ry1}{wx1}
      \end{tikzinline}}
  \end{gather*}
This candidate execution forms a cycle in causal order, so is disallowed,
but this depends crucially on the control dependency
from $(\DR{x}{1})$ to $(\DW{y}{1})$, and the data dependency
from $(\DR{y}{1})$ to $(\DW{x}{1})$. If either is missing, then this execution
is acyclic and hence allowed. For example dropping the control dependency
results in:
  \begin{gather*}
    \THREAD{r\GETS x\SEMI y\GETS1}
    \PAR
    \THREAD{s\GETS y\SEMI x\GETS s}
    \\[-.4ex]
    \nonumber
    \hbox{\begin{tikzinline}[node distance=1.5em]
        \event{rx1}{\DR{x}{1}}{}
        \event{wy1}{\DW{y}{1}}{right=of rx1}
        \event{ry1}{\DR{y}{1}}{right=of wy1}
        \event{wx1}{\DW{x}{1}}{right=of ry1}
        \rf[out=20,in=160]{wy1}{ry1}
        \rf[out=160,in=20]{wx1}{rx1}
        \po{ry1}{wx1}
      \end{tikzinline}}
  \end{gather*}
Unfortunately, while a simple syntactic approach to dependency calculation
suffices for hardware models, it is not preserved by common compiler
optimizarions. For example, if we calculate control dependencies syntactically,
then there is a dependency from $(\DR{x}{1})$ to $(\DW{y}{1})$, and therefore a cycle in, the candidate execution:
  \begin{gather*}
    \THREAD{r\GETS x\SEMI \IF r \THEN y\GETS1 \ELSE y\GETS1 \FI}
    \PAR
    \THREAD{s\GETS y\SEMI x\GETS s}
    \\[-.4ex]
    \nonumber
    \hbox{\begin{tikzinline}[node distance=1.5em]
        \event{rx1}{\DR{x}{1}}{}
        \event{wy1}{\DW{y}{1}}{right=of rx1}
        \event{ry1}{\DR{y}{1}}{right=of wy1}
        \event{wx1}{\DW{x}{1}}{right=of ry1}
        \rf[out=20,in=160]{wy1}{ry1}
        \rf[out=160,in=20]{wx1}{rx1}
        \po{rx1}{wy1}
        \po{ry1}{wx1}
      \end{tikzinline}}
  \end{gather*}
An optimizing compiler might lift the assignment $y\GETS1$ out of the conditional,
thus removing the control dependency.

Prominent solutions to the problem of dependency calculation include:
\begin{itemize}

\item \emph{syntactic} methods used in hardware models
  such as ARM~\cite{???} or x86-TSO~\cite{???},
\item \emph{alternate universe} methods (which give a semantics based on multiple speculative executions
  of the same program) such as the Java Memory Model~\cite{???},
  the speculative operational semantics of~\cite{???},
  the promising semantics of~\cite{???},
  or the event structures semantics of~\cite{???},
\item \emph{rewriting} methods, which give an operational model
  up to syntactic rewrites, such as~\cite{???}, and
\item \emph{logical} methods, such as the pomsets with preconditions
  model of~\cite{???}.
  
\end{itemize}
In this paper, we will focus on logical models, as those are compositional,
and align well with existing models of sequential composition.

\subsection{Sequential composition}

%% Program logics and semantics tell us that when executing ((S1; S2), state0),
%% we execute (S1, state0) to arrive at state1, then execute (S2, state1) to
%% arrive at the final state2.

%% This is a delightfully simple story that can be explained to children.  It is
%% also a lie.

%% Processors execute instructions out of order, due to pipelines and caches.
%% Compilers reorder programs even more dramatically.  All of this reordering is
%% meant to be unobservable in single-threaded code.  In multi-threaded code,
%% however, all bets are off.  A formal attempt to understand the resulting mess
%% is known as a ``relaxed memory model.''

%% Most of the relaxed memory models that have been proposed are designed to
%% help us understand whole program execution: they have no compositionality
%% properties whatsoever.  Recently, denotation models have appeared that treat
%% \emph{concurrent} execution compositionally.  One such model is ``Pomsets with
%% Preconditions''.  It remains an open question, however, whether it is
%% possible to treat \emph{sequential} execution compositionally in such a model,
%% without overly restricting processors and compilers.

%% We propose adding families of predicate transformers to Pomsets with
%% Preconditions.  The resulting model is denotational, supporting both parallel
%% and sequential composition.  When composing (S1;S2), the predicate
%% transformer used to validate the precondition of an event in S2 is chosen
%% based on the dependency order from S1 into this event.  As usual in work on
%% relaxed memory, we have not handled loops or recursion.

%% Happily, most of the results expected of a relaxed memory model can be
%% established by appeal to prior work.  So here we are able to concentrate on
%% the model itself.  The model is formalized in Agda, where we have established
%% associativity for sequential composition.

%% For the memory-model specialist, we retain the good properties of the prior
%% work on Pomsets with Preconditions, fixing some errors along the way.  These
%% properties include efficient implementation on ARMv8, support for compiler
%% optimizations, support for logics that prove the absence of thin-air
%% behaviors, and a local data race freedom theorem.

% \begin{figure*}
%   \input{fig-sp}
%   \caption{Weakest Precondition and Strongest Postcondition \cite{}}
% \end{figure*}


Our approach follows that of weakest precondition semantics of
\citet{DBLP:journals/cacm/Dijkstra75}, which provides an alternative
characterization of Hoare logic \citep{Hoare:1969:ABC:363235.363259} by
mapping postconditions to preconditions.

% Our transformers $\aTr$ map preconditions to postconditions, in order to
% apply to preconditions.
% \begin{align*}
%   \hoare{\aForm}{\bCmd}{\bForm}
%   \;\;\Leftrightarrow\;\;
%   \hoare{\aTr(\aCmd,\aForm)}{\aCmd\SEMI\bCmd}{\bForm}
% \end{align*}
% For example
% \begin{gather*}
%   \hoare{r{=}1}{\PW{y}{r}}{y{=}1}
%   \\
%   \hoare{x{=}1}{\PR{x}{r}\SEMI\PW{y}{r}}{y{=}1}
% \end{gather*}


\endinput

This paper builds on
\cite{2019-sp} and
\cite{10.1145/3428262}.


Pomsets are a nice model of concurrent computation.
Predicate transformers are a nice model of sequential computation.
We show how these interact.

We do this by attaching predicate transformers to \emph{configurations}.

Thus the amount of transformation depends on which read events are seen in
the configuration.

We develop the theory of pomsets with predicate transformers, then apply the
theory to model relaxed memory.

Other things we could model include Map/Reduce and Fork/Join.
